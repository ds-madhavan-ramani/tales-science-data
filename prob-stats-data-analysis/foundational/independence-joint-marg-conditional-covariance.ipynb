{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence; joint/marginal/conditional probability; covariance and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical independence\n",
    "\n",
    "Two random variables $X$ and $Y$ are said to be *independent* when their joint probability is equal to the product of the probabilities of each:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X) P(Y) \\ . \n",
    "$$\n",
    "\n",
    "This means, in terms of conditional probabilities,\n",
    "\n",
    "$$\n",
    "P(X | Y) = \\frac{P(X, Y)}{P(Y)} = \\frac{P(X)P(Y)}{P(Y)} = P(X) \\ ,\n",
    "$$\n",
    "\n",
    "that is, the probability of $X$ occurring is not affected by the occurring of $Y$. This is typically how independence is defined, in word terms: the occurrence of one event does not influence the occurrence of the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IID variables\n",
    "\n",
    "I.I.D. stands for *independent* and *identically distributed*, it's a shortening used all over in statistics. IID variables are [independent](independence.ipynb) but also distributed in the same way. \n",
    "\n",
    "The concept is the basic assumptions of many foundational results in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The joint probability\n",
    "\n",
    "The joint probability of one or more events is the probability that they happen together. If $X$, $Y$, $Z$, ... are the random variables, their joint probability is written as\n",
    "\n",
    "$$\n",
    "P(X, Y, Z, \\ldots)\n",
    "$$\n",
    "\n",
    "or as \n",
    "\n",
    "$$\n",
    "P(X \\cap Y \\cap Z \\ldots)\n",
    "$$\n",
    "\n",
    "### The case of independent variables\n",
    "\n",
    "If the variables are independent, their joint probability reduces to the product of their probabilities: $P(X_1, X_2, \\ldots, X_n) = \\Pi_{i=1}^n P(X_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The marginal probability\n",
    "\n",
    "\n",
    "<figure style=\"float:left;\">\n",
    "  <img src=\"../../imgs/joint-marg.png\" width=\"500\" align=\"left\" style=\"margin:0px 50px\"/>\n",
    "  <figcaption>Image by IkamusumeFan (Own work) [<a href=\"http://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>], <a href=\"https://commons.wikimedia.org/wiki/File%3AMultivariate_normal_sample.svg\">via Wikimedia Commons</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "If we have the joint probability of two or more random variables, the marginal probability of each is the probability related to that variable and to its own space of events; it expresses the probability of the variable when the value of the other one is not known. It calculated by summing the joint probability over the space of events of the other variable. More specifically, given $P(X, Y) = P(X=x, Y=y)$,\n",
    "\n",
    "$$\n",
    "P(X=x) = \\sum_y P(X=x, Y=y) \\ .\n",
    "$$\n",
    "\n",
    "The illustration here shows points extracted from a joint probability (the black dots) and the marginal probabilities as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The conditional probability\n",
    "\n",
    "The conditional probability expresses the probability that an event occurrs given that another one has occurred. With $Y$ being the (variable related to the) event that has occurred and $X$ the (variable related to the) event whose probability of occurrence we are interested in, it is defined as\n",
    "\n",
    "$$\n",
    "P(X | Y) = \\frac{P(X, Y)}{P(Y)} \\ ,\n",
    "$$\n",
    "\n",
    "that is, as the ratio of the joint probability of the two to the probability of $Y$. \n",
    "\n",
    "In the case of more than two variables we can write the joint probability as\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, \\ldots, X_n) = P(X_1 | X_2, \\ldots, X_n) P(X_2, \\ldots, X_n)\n",
    "$$\n",
    "\n",
    "and can repeat the process to isolate them one by one, obtaining\n",
    "\n",
    "$$\n",
    "P(\\cap_{i=1}^n X_i) = \\Pi_{i=1}^n P(X_i | \\cap_{j=i+1}^{n} X_j) \\ ,\n",
    "$$\n",
    "\n",
    "which is known as the [chain rule](https://en.wikipedia.org/wiki/Chain_rule_(probability). \n",
    "\n",
    "### In the case of independence\n",
    "\n",
    "This is easy, we would have\n",
    "\n",
    "$$\n",
    "P(X | Y) = \\frac{P(X, Y)}{P(Y)} = \\frac{P(X) P(Y)}{P(Y)} = P(X) \\ ,\n",
    "$$\n",
    "\n",
    "which indicates what is suggested by the definition itself: independent variables mean that the happening of one does not influence the happening of the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Covariance and correlation\n",
    "\n",
    "### Covariance\n",
    "\n",
    "Given the random variables $X$ and $Y$ with respective means $\\mu_x$ and $\\mu_y$, their *covariance* is defined as\n",
    "\n",
    "$$\n",
    "\\text{cov}(X, Y) = \\mathbb{E}[(X - \\mu_x)((Y - \\mu_y)]\n",
    "$$\n",
    "\n",
    "It is a measure of how jointly the two variables vary: a positive covariance means that when $X$ grows, $Y$ grows as well and a negative covariance means that when $X$ grows, $Y$ decreases. \n",
    "                                          \n",
    "### Correlation\n",
    "\n",
    "The word *correlation* is measured by a *correlation coefficient* which exists in several definitions depending on what is exactly measured; it is always a sort of normalised covariance. \n",
    "The correspondent of the covariance itself is Pearson's definition, which defines the correlation coefficient as the covariance normalised by the product of the standard deviations of the two variables:\n",
    "\n",
    "$$\n",
    "\\rho_{xy} = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y} = \\frac{\\mathbb{E}[(x - \\mu_x)(y - \\mu_y)]}{\\sigma_x \\sigma_y} \\ ,\n",
    "$$\n",
    "\n",
    "and it can also be written as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\rho_{xy} &= \\frac{\\mathbb{E}[(xy - x \\mu_y - \\mu_x y + \\mu_x \\mu_y)]}{\\sigma_x \\sigma_y} \\\\\n",
    "    &= \\frac{\\mathbb{E}[xy] - \\mu_x\\mu_y - \\mu_y\\mu_x + \\mu_x\\mu_y}{\\sigma_x \\sigma_y} \\\\\n",
    "    &= \\frac{\\mathbb{E}[xy] - \\mu_x\\mu_y}{\\sigma_x \\sigma_y} \\ .\n",
    "\\end{align}\t\t\t\n",
    "$$\n",
    "\n",
    "The correlation coefficient has these properties:\n",
    "\n",
    "* $-1 \\leq \\rho_{xy} \\leq 1$\n",
    "* It is symmetric: $\\rho_{xy} = \\rho_{yx}$\n",
    "* If the variables are independent, then $\\rho_{xy} = 0$ (but the reverse is not true)\n",
    "\n",
    "### Independence and correlation\n",
    "\n",
    "Let's expand on the last point there really. We said that if two random variables are independent, then the correlation coefficient is zero. This is easy to prove as it follows directly from the definition above (also bear in mind [Fubini's theorem](https://en.wikipedia.org/wiki/Fubini's_theorem)):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[XY] = \\int_{\\Omega_X } \\int_{\\Omega_Y} \\text{d} x \\text{d} y \\ xy P(x,y) = \\int_{\\Omega_X } \\int_{\\Omega_Y} \\text{d} x \\text{d} y \\ xy P(x) P(y) = \\mu_x \\mu_y \\ .\n",
    "$$\n",
    "\n",
    "The reverse is not true. Look at this amazing Q&A on [Cross Validated](https://stats.stackexchange.com/questions/12842/covariance-and-independence#) for a well explained counter-example.\n",
    "\n",
    "### Correlation and the relation between variables\n",
    "\n",
    "<img src=\"../../imgs/correlation-ex.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Correlation says \"how much\" it happens that when $x$ grows, $y$ grows as well. It is not a measure of the slope of the linear relation between $x$ and $y$. This is greatly illustrated in the figure above (from Wikipedia's [page](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)), which reports sets of data points with $x$ and $y$ and their correlation coefficient. \n",
    "\n",
    "In the center figure, because the variance of $y$ is 0, then the correlation is undefined. In the bottom row, the relation between variables is not linear, the correlation does not capture that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
